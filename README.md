# Aubrushli_images
An Image Generator utilising Stable Diffusion

* Very much a work in progress, no guarantees at all.

Aubrushli Images takes a shot list and cast list generated by Aubrushli and creates images that can be used by the storyboard part of Aubrushli, but the images are just normal PNGs so can be used maybe for a pitchdeck or set inspiration or for whatever.
Stable Diffusion itself and any of its front ends have a million more features than this, however I aim to give this over time the specific goal of turning a shot list into a storyboard almost automatically and that means focussing on the transferring script language and interpreting it
in a prompt. 

The actual integration with stable diffusion is pretty light and therefore any updates or perhaps utilising other image generators in the future should be fairly easy to incorporate.

# So what does this do?

Well the process is:- create a production name, choose the number of images per shot, associate an aubrushli formatted Shot list, cast list and save folder with that production. Associate Characters with celebrities (although you could use whatever you want) to allow more consistent images.
Then choose which shots to create images for, and wait (hopefully not too long but it depends on your hardware)

# What it won't let you do

Most stable diffusion things. Not even changing CFG_Scale (I leave at default, the thing is I figure if you are using for storyboards you just want to use it and not get too involved in the frankly a bit unpredictable cfg_scale). At the moment Image aspect ratio is wider than the normal 512 x 512 (if not quite 16:9). This is because this is aimed at storyboards and most will be wide screen. This may well cause issues on lower hardware. I havent added any controlnet features or actually any img to img because the idea is to get it directly from a shot list. Thats not to say it won't look a million times better so maybe later I will give tha option. I looked at Gligen and at latent couple as I thought they could be very useful but right now if I use a wider format they tend to create multiples much more than normal. I want to properly incorporate Seeds as I think that will be useful for a consistent look across shots and that is my very next thing to do.


## Brief suggested install process

+ Create a conda environment
+ Activate
+ git clone the above repo to a local folder or if you have a SD local installation there should work without having to download loads of stuff.
+ pip install PyQt5
+ conda install pandas
+ conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia ******DEPENDS ON SYSTEM BUT INSTALL PYTORCH******
+ pip install diffusers
+ pip install git+https://github.com/huggingface/transformers ******************NEED TRANSFORMERS FROM SOURCE ******************
+ pip install accelerate

Then you need to go and get folders and files from Stability AI. The one I used was at hugging face https://huggingface.co/stabilityai/stable-diffusion-2-1/tree/main

However I realise that I can reduce this down substantially by not using the safetensors files:- below is the folders and files I am using. Most of them are from that huggingface repository but the really cut back model is from https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.ckpt and the qss sheet is from ... 

![AUb_images](https://user-images.githubusercontent.com/26924183/229891129-cbdc51c9-782f-44a2-908d-8fdac2ad46ab.png)


Anyway thats the initial todo, lots to add. 
